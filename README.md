# data-pipelines-airflow-app
    As a part of this project we are demonstrating how to build the data pipelines using apache airflow as engineering 
    orchestration tool airflow-data-pipeline

# Introduction
    A music streaming company, Sparkify, has decided that it is time to introduce more automation and monitoring to 
    their data warehouseETL pipelines and come to the conclusion that the best tool to achieve this is Apache Airflow.
    The source data resides in S3 and needs to be processed in Sparkify's data warehouse in Amazon Redshift. 
    The source datasets consist of JSON logs that tell about user activity in the application and JSON metadata about 
    the songs the users listen to.

# Tools used:
    Install Python3
    Install Docker
    Udacity or Free tier AWS account and Redshift cluster
  ## Clone repository to local machine
     https://github.com/sp3006/data-pipeline-airflow-app.git
     Change directory to local repository
     cd data-pipeline-airflow-app
# Create python virtual environment
      python3 -m venv venv             # create virtualenv
      source venv/bin/activate         # activate virtualenv
      pip install -r requirements.txt  # install requirements
      If you are using python older version please use virtualenv command to create the virtual env.
  


